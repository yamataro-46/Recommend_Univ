{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a207ff09",
   "metadata": {},
   "source": [
    "# 3年秋プロジェクト\n",
    "## 自然言語処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa104a67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# サイトから特定の文章を取得する\n",
    "# ※※サーバー攻撃と処理時間の問題点からこれは実行しない※※\n",
    "\n",
    "\n",
    "# みんなの大学情報\n",
    "\n",
    "# MARCH大学群\n",
    "univs_name = {\n",
    "    20293: 'meiji',\n",
    "    20192: 'aoyama',\n",
    "    20298: 'rikkyo',\n",
    "    20241: 'tyuo',\n",
    "    20287: 'hosei'\n",
    "}\n",
    "# 各大学の学部リスト\n",
    "univs_groups = {\n",
    "    20293: [824, 825, 826, 827, 828, 829, 830, 831, 832, 833],\n",
    "    20192: [469, 470, 471, 472, 473, 474, 475, 476, 477, 2279, 2563],\n",
    "    20298: [854, 855, 856, 857, 858, 859, 860, 861, 862, 863],\n",
    "    20241: [656, 657, 658, 659, 660, 661, 2565, 2566],\n",
    "    20287: [794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 2328]\n",
    "}\n",
    "# 取り出したいクラス\n",
    "Class = '.mod-reviewList-txt'\n",
    "\n",
    "\n",
    "def univ_reviews(univ, group):\n",
    "    '''レビューサイトをスクレイピングし特定のクラスを取得'''\n",
    "    \n",
    "    reviews = ''\n",
    "    p = 1\n",
    "    while True:\n",
    "        url = 'https://www.minkou.jp/university/school/review/' + str(univ) + '/' + str(group) + '/page=' + str(p) + '/'\n",
    "        html = requests.get(url)\n",
    "        contents = BeautifulSoup(html.content, \"html.parser\")\n",
    "        if len(contents.select(Class)) == 0:\n",
    "            break\n",
    "        for text in contents.select(Class):\n",
    "            reviews += text.getText()\n",
    "        p += 1\n",
    "        time.sleep(1)\n",
    "    return reviews\n",
    "\n",
    "def univs_reviews(univs):\n",
    "    '''各大学の各学部のレビューを取得し辞書型をファイルに保存'''\n",
    "    \n",
    "    for univ in univs:\n",
    "        print(univs_name[univ], univ)\n",
    "        univ_revs = {}\n",
    "        for group in univs[univ]:\n",
    "            print(group)\n",
    "            # 1つの大学に対して、各学部のレビューを取得\n",
    "            univ_revs[group] = univ_reviews(univ, group)\n",
    "        # 取得したレビューを各大学ごとのpickleファイルに辞書型で保存\n",
    "        with open(univs_name[univ]+\".pkl\", \"wb\") as f:\n",
    "            pickle.dump(univ_revs, f)\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期準備\n",
    "# import, リスト・辞書の定義\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from janome.tokenizer import Tokenizer\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "from gensim import models\n",
    "import pickle\n",
    "\n",
    "\n",
    "# 以下手動で入力　自動取得法を模索 class=\"schMod-departmentList-name\"でとってこれそう\n",
    "univs_name = {\n",
    "    20293: ['meiji', '明治大学'],\n",
    "    20192: ['aoyama', '青山学院大学'],\n",
    "    20298: ['rikkyo', '立教大学'],\n",
    "    20241: ['tyuo', '中央大学'],\n",
    "    20287: ['hosei', '法政大学']\n",
    "}\n",
    "undergraduates_name = {\n",
    "    824: '法学部', 825: '政治経済学部', 826: '経営学部', 827: '商学部', 828: '文学部', 829: '情報コミュニケーション学部', \n",
    "    830: '理工学部', 831: '農学部', 832: '国際日本学部', 833: '総合数理学部',\n",
    "    469: '法学部', 470: '経済学部', 471: '経営学部', 472: '文学部', 473: '国際政治経済学部', 474: '教育人間科学部', \n",
    "    475: '総合文化政策学部', 476: '理工学部', 477: '社会情報学部', 2279: '地球社会共生学部', 2563: 'コミュニティ人間科学部',\n",
    "    854: '法学部', 855: '経済学部', 856: '文学部', 857: '理学部', 858: '社会学部', 859: '経営学部', \n",
    "    860: '異文化コミュニケーション学部', 861: '観光学部', 862: 'コミュニティ福祉学部', 863: '現代心理学部',\n",
    "    656: '法学部', 657: '経済学部', 658: '商学部', 659: '文学部', 660: '総合政策学部', 661: '理工学部',\n",
    "    2565: '国際経営学部', 2566: '国際情報学部',\n",
    "    794: '法学部', 795: '経営学部', 796: '文学部', 797: '国際文化学部', 798: '人間環境学部', 799: 'キャリアデザイン学部',\n",
    "    800: 'デザイン工学部', 801: '情報科学部', 802: '理工学部', 803: '生命科学部', 804: 'スポーツ健康学部', \n",
    "    805: '経済学部', 806: '現代福祉学部', 807: '社会学部', 2328: 'グローバル教養学部',\n",
    "}\n",
    "univs_groups = {\n",
    "    20293: [824, 825, 826, 827, 828, 829, 830, 831, 832, 833],\n",
    "    20192: [469, 470, 471, 472, 473, 474, 475, 476, 477, 2279, 2563],\n",
    "    20298: [854, 855, 856, 857, 858, 859, 860, 861, 862, 863],\n",
    "    20241: [656, 657, 658, 659, 660, 661, 2565, 2566],\n",
    "    20287: [794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 2328]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルの読み込み\n",
    "\n",
    "# ローカルに保存した法政のレビュー解析結果(pklファイル)を読み込む\n",
    "with open('meiji.pkl', 'rb') as f:\n",
    "    meiji_revs = pickle.load(f)\n",
    "\n",
    "with open('aoyama.pkl', 'rb') as f:\n",
    "    aoyama_revs = pickle.load(f)\n",
    "\n",
    "with open('rikkyo.pkl', 'rb') as f:\n",
    "    rikkyo_revs = pickle.load(f)\n",
    "\n",
    "with open('tyuo.pkl', 'rb') as f:\n",
    "    tyuo_revs = pickle.load(f)\n",
    "\n",
    "with open('hosei.pkl', 'rb') as f:\n",
    "    hosei_revs = pickle.load(f)\n",
    "    \n",
    "univs_revLsit = [meiji_revs, aoyama_revs, rikkyo_revs, tyuo_revs, hosei_revs]\n",
    "\n",
    "\n",
    "\n",
    "# 各大学、各学部のレビューにおける形態素解析済みの単語リストと単語出現回数リスト\n",
    "# pklに保存しているものをopenする\n",
    "with open('revw.pkl', 'rb') as f:\n",
    "    revw = pickle.load(f)\n",
    "\n",
    "with open('revwc.pkl', 'rb') as f:\n",
    "    revwc = pickle.load(f)\n",
    "\n",
    "# レビューの分かち書きリスト\n",
    "with open('wakati_docs.pkl', 'rb') as f:\n",
    "    wakati_docs = pickle.load(f)\n",
    "\n",
    "# 大学学部のレビューの学習モデル\n",
    "rev_model = models.Word2Vec.load('univ_review.model')\n",
    "\n",
    "# 白ヤギモデル・大学学部レビューモデルの融合学習モデルの読み込み\n",
    "fus_model = models.Word2Vec.load('gensim_univs.model')\n",
    "\n",
    "# 大学学部と各レビューのdoc2vecモデル\n",
    "revs_model = models.Doc2Vec.load('univs_reviews.model')\n",
    "\n",
    "# 学習させる文書をレビュー全文の分かち書きとしたdoc2vecモデル\n",
    "wakati_model = models.Doc2Vec.load('wakati_reviews.model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c977ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数等定義\n",
    "\n",
    "def tf(words):\n",
    "    tf_words = {}\n",
    "    s = sum(words.values())\n",
    "    for key, value in words.items():\n",
    "        tf_words[key] = value / s\n",
    "    return tf_words\n",
    "\n",
    "def df(messages):\n",
    "    df_words = {}\n",
    "    for prof, mesg in messages.items():\n",
    "        for key in mesg.keys():\n",
    "            num = df_words.get(key)\n",
    "            if num == None:\n",
    "                df_words[key] = 1\n",
    "            else:\n",
    "                df_words[key] = num + 1\n",
    "    return df_words\n",
    "\n",
    "def tfidf(messages):\n",
    "    df_words = df(messages)\n",
    "    tf_tfidf = {}\n",
    "    size = len(messages)\n",
    "    for prof, mesg in messages.items():\n",
    "        tf_scores = tf(mesg)\n",
    "        tfidf_scores = {}\n",
    "        for key, value in tf_scores.items():\n",
    "            tfidf_scores[key] = value * math.log2(size / df_words[key])\n",
    "        tf_tfidf[prof] = (tf_scores, tfidf_scores)\n",
    "    return tf_tfidf\n",
    "\n",
    "\n",
    "\n",
    "# 大学学部名のリスト化(最後尾はテスト用)\n",
    "# 各レビュー文の分かち書きリスト(文書作成) tfidfの値が0の単語をストップワードにして、それ以外をリストに追加する\n",
    "# 結果から、「青山学院大学コミュニティ人間科学部」だけ38個であり、データ数が他と比べて少なすぎる\n",
    "# 青山学院大学コミュニティ人間科学部はレビューが少なすぎるため今回は除外\n",
    "univsName = []\n",
    "docs = []\n",
    "stop_words = []\n",
    "for i, u in enumerate(univs_groups):\n",
    "    univ = univs_name[u][1]\n",
    "    for g in univs_groups[u]:\n",
    "        if g == 2563: continue\n",
    "        undergraduate = undergraduates_name[g]\n",
    "        name = univ + ' ' + undergraduate\n",
    "        univsName.append(name)\n",
    "\n",
    "        doc = []\n",
    "        stop_word = []\n",
    "        tf_idf = tfidf(revwc[i])[g][1]\n",
    "        tf_idf_keys = list(tf_idf.keys())\n",
    "        tf_idf_values = list(tf_idf.values())\n",
    "        for k in range(len(tf_idf)):\n",
    "            if tf_idf_values[k] == 0.0:\n",
    "                stop_word.append(tf_idf_keys[k])\n",
    "                stop_words.append(stop_word)\n",
    "        doc = [d for d in revw[i][g] if d not in stop_word]\n",
    "        docs.append(doc)\n",
    "\n",
    "\n",
    "# 各学部のレビュー全文を分かち書きしてリスト化\n",
    "# wakati_docs = []\n",
    "# tw = Tokenizer(wakati=True)\n",
    "# for i, univ in enumerate(univs_groups):\n",
    "#     for g in univs_groups[univ]:\n",
    "#         if g == 2563: continue\n",
    "#         token = tw.tokenize(univs_revLsit[i][g])\n",
    "#         wakati_doc = list(token)\n",
    "#         wakati_docs.append(wakati_doc)\n",
    "\n",
    "\n",
    "# 入力文を分かち書きする\n",
    "def wakati_sentence(text):\n",
    "    tw = Tokenizer(wakati=True)\n",
    "    token = tw.tokenize(text)\n",
    "    tokenList = list(token)\n",
    "    return tokenList\n",
    "\n",
    "\n",
    "# 文章を形態素解析(分かち書き)する　単語群と出現回数リストを返す関数\n",
    "def wakati_count_words(contents):\n",
    "    t1 = Tokenizer()\n",
    "    words = []\n",
    "    words_count = {}\n",
    "    for token in t1.tokenize(contents):\n",
    "        parts = token.part_of_speech.split(',')\n",
    "        if('名詞' in parts\n",
    "            or '動詞' in parts\n",
    "            or '形容詞' in parts\n",
    "            or '形容動詞' in parts):\n",
    "            base = token.base_form\n",
    "            words.append(base)\n",
    "            rec = words_count.get(base)\n",
    "            if rec == None:\n",
    "                words_count[base] = 1\n",
    "            else:\n",
    "                words_count[base] = rec + 1\n",
    "    return words, words_count\n",
    "\n",
    "\n",
    "# 白ヤギ+レビューのword2vecモデルを用いて、単語の似ているワードをリスト化\n",
    "def similar_words(text, model, n):\n",
    "    sentence = wakati_sentence(text)\n",
    "    words, words_count = wakati_count_words(text)\n",
    "    wordsList = sentence\n",
    "    for word in words:\n",
    "        sim_words = model.wv.most_similar(word, topn=n)\n",
    "        for w in range(len(sim_words)):\n",
    "            wordsList.append(sim_words[w][0])\n",
    "    return wordsList\n",
    "\n",
    "\n",
    "\n",
    "# レビューに大学学部名を付与するためのクラス\n",
    "class LabeledListSentence(object):\n",
    "    def __init__(self, words_list, labels):\n",
    "        self.words_list = words_list\n",
    "        self.labels = labels\n",
    "    def __iter__(self):\n",
    "        for i, words in enumerate(self.words_list):\n",
    "            yield models.doc2vec.LabeledSentence(words, ['%s' % self.labels[i]])\n",
    "\n",
    "\n",
    "# docsに入力となるテキストを新たに追加して、それを込みで学習させる\n",
    "def testData_train(data, docs, univs):\n",
    "    univs.append('test')\n",
    "    docs.append(data)\n",
    "    sentences = LabeledListSentence(docs, univs)\n",
    "    model = models.Doc2Vec(dm=1, size=200, min_count=5, alpha=0.025, iter=20, workers=4)\n",
    "    model.build_vocab(sentences)\n",
    "    model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    univs = model.docvecs.offset2doctag\n",
    "    univs.pop()\n",
    "    docs.pop()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# 入力文から、最も近い・合っている大学学部を出力(提案)する\n",
    "def output_similar_univ(input_text, model, n):\n",
    "    text = similar_words(input_text, fus_model, 10)\n",
    "\n",
    "    # 新規文書のモデルにおけるベクトルを作成する\n",
    "    input_vec = model.infer_vector(text)\n",
    "    \n",
    "    # 新規文書のベクトルと近いものを出力 ⇒ 条件や要件に近い・合っている大学学部を提案する\n",
    "    output_univs = model.docvecs.most_similar([input_vec], topn=n)\n",
    "\n",
    "    return output_univs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b8a0e",
   "metadata": {},
   "source": [
    "#### 実験"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69738842",
   "metadata": {},
   "source": [
    "label:test, doc:入力文　を追加したsentencesを再学習して近いベクトルを比較する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb93435",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'プログラミングを学べる、CG、音声処理、画像処理、情報、パソコンに詳しくなりたい、設備がきれい'\n",
    "text = similar_words(input_text, fus_model, 10)\n",
    "test_model = testData_train(text, docs, univsName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10530ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 54の文書が入力したテキスト文\n",
    "# 理論上は、こちらが入力した条件(text)に最も近い(適切な)大学学部を出力する\n",
    "# 精度は不明\n",
    "\n",
    "test_model.docvecs.most_similar(53, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a23a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.docvecs.most_similar(0, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ccadc6",
   "metadata": {},
   "source": [
    "2022.1.10　精度が低い\n",
    "考えられる原因や改善点\n",
    "・Doc2Vecの文書比較の中身や計算方法を理解しきれていないため最適ではない可能性\n",
    "・Doc2Vecのパラメータの変更やエポック数分の繰り返し学習の実行\n",
    "・各大学学部の文書となるdocsの中身を見たところ、ゴミが多い(思う、する、ある、数字や記号...)\n",
    "　→その単語単体では意味を持たないような、もしくはその学部を特徴づける語ではないものが多いため、精度が下がっているのでは\n",
    "　→現在のdocsの作成方法はレビューを名詞・動詞・形容詞・形容動詞で分かち書きしているだけ\n",
    "　→出現回数やtf-idfでふるいにかけることで精度は向上するかもしれない、ストップワードの活用など"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c32e98e",
   "metadata": {},
   "source": [
    "Doc2Vecのライブラリを調べたりして、最適な手法を模索する\n",
    "\n",
    "infer_vectorで未知の入力文(今回のtext)をモデルにおけるベクトルで表す\n",
    "得られたベクトルに近いものを出力する\n",
    "\n",
    "多分、上記のtestData_trainでやるよりも、適切な手法だと思われる。\n",
    "testdata分の文書やベクトルが他の学部の結果に影響してしまうから。\n",
    "\n",
    "ただ、やはり精度は十分とは言えず、docsの修正が必須だと考えられる\n",
    "\n",
    "1/11 0:10 docsを修正　tfidfの処理で特徴語でないものを除いた分かち書きリスト(ゴミを削除)\n",
    "\n",
    "\n",
    "今後の改善点としては\n",
    "doc2vecモデルの学習精度を上げるためにパラメータ等を工夫する\n",
    "docsはゴミを消したので、基本的にこれ以上は無理と仮定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f974fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 理科大　理工学部\n",
    "test_doc = univ_reviews(20267, 729)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efb6fe6",
   "metadata": {},
   "source": [
    "各学部のmodelを作成して、infer_vectorによるベクトル生成で比較する方法\n",
    "\n",
    "docs は名詞動詞形容詞形容動詞を取得しtf-idfで特徴のない語を除去したリスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b253ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大学学部とレビューのDoc2Vecのモデル構築と学習\n",
    "\n",
    "sentences = []\n",
    "for i in range(len(univsName)):\n",
    "    doc = docs[i]\n",
    "    tag = [univsName[i]]\n",
    "    sentences.append(models.doc2vec.TaggedDocument(doc, tag))\n",
    "\n",
    "revs_model = models.Doc2Vec(sentences, dm=1, size=200, min_count=5, alpha=0.025, iter=20, workers=4)\n",
    "\n",
    "# sentences = LabeledListSentence(docs, univsName)\n",
    "# revs_model = models.Doc2Vec(size=300, dm=1, window=10, alpha=0.025, min_count=5, iter=20, workers=4)\n",
    "# revs_model.build_vocab(sentences)\n",
    "# revs_model.train(sentences, total_examples=revs_model.corpus_count, epochs=revs_model.iter)\n",
    "\n",
    "revs_model.save('univs_reviews.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e4ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大学学部と各レビューのdoc2vecモデル\n",
    "revs_model = models.Doc2Vec.load('univs_reviews.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea135e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'プログラミングを学べる、CG、音声処理、画像処理、情報、パソコンに詳しくなりたい、設備がきれい'\n",
    "output_similar_univ(input_text, revs_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8921e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('「課題」と似ている単語')\n",
    "print(revs_model.most_similar('課題'))\n",
    "print()\n",
    "print('「プログラミング」と似ている単語')\n",
    "print(revs_model.most_similar('プログラミング'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a15e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('「明治大学法学部」と似ている大学学部')\n",
    "print(revs_model.docvecs.most_similar(0))\n",
    "print()\n",
    "print('「法政大学情報科学部」と似ている大学学部')\n",
    "print(revs_model.docvecs.most_similar(45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64678a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = wakati_sentence(test_doc)\n",
    "\n",
    "# 新規文書のモデルにおけるベクトルを作成する\n",
    "input_vec = revs_model.infer_vector(text)\n",
    "\n",
    "# 新規文書のベクトルと近いものを出力 ⇒ 条件や要件に近い・合っている大学学部を提案する\n",
    "revs_model.docvecs.most_similar([input_vec], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd41d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd495b85",
   "metadata": {},
   "source": [
    "docs をレビュー全文の分かち書きにして実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d270e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for i in range(len(univsName)):\n",
    "    doc = wakati_docs[i]\n",
    "    tag = [univsName[i]]\n",
    "    sentences.append(models.doc2vec.TaggedDocument(doc, tag))\n",
    "\n",
    "wakati_model = models.Doc2Vec(sentences, dm=1, size=200, min_count=5, alpha=0.025, iter=20, workers=4)\n",
    "\n",
    "wakati_model.save('wakati_reviews.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fd6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "wakati_model = models.Doc2Vec.load('wakati_reviews.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee2021",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('「課題」と似ている単語')\n",
    "print(wakati_model.most_similar('課題'))\n",
    "print()\n",
    "print('「プログラミング」と似ている単語')\n",
    "print(wakati_model.most_similar('プログラミング'))\n",
    "print()\n",
    "print()\n",
    "print('「明治大学法学部」と似ている大学学部')\n",
    "print(wakati_model.docvecs.most_similar(0))\n",
    "print()\n",
    "print('「法政大学情報科学部」と似ている大学学部')\n",
    "print(wakati_model.docvecs.most_similar(45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'プログラミングを学べる大学。CGとか音声処理とか画像処理の分野を勉強したい。IT系。パソコンに詳しくなりたい。'\n",
    "\n",
    "print('「', input_text, '」 という条件に近い大学学部')\n",
    "print()\n",
    "output_similar_univ(input_text, wakati_model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = '偏差値そこそこ。広い、綺麗。おしゃれ。課題が少ない。教授が有名。友達が作りやすい。男女比のバランスが良い。ネームバリューが良い。就職に有利、就職先が大手。対面授業。立地が良い。'\n",
    "\n",
    "print('「', input_text, '」 という条件に近い大学学部')\n",
    "print()\n",
    "output_similar_univ(input_text, wakati_model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ddf5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = wakati_sentence(test_doc)\n",
    "\n",
    "# 新規文書のモデルにおけるベクトルを作成する\n",
    "input_vec = wakati_model.infer_vector(text)\n",
    "\n",
    "# 新規文書のベクトルと近いものを出力 ⇒ 条件や要件に近い・合っている大学学部を提案する\n",
    "wakati_model.docvecs.most_similar([input_vec], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a4cbae",
   "metadata": {},
   "source": [
    "## 以下各手法まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85519a4",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e574501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wakati_count_words(contents):\n",
    "    t1 = Tokenizer()\n",
    "    words = []\n",
    "    words_count = {}\n",
    "    for token in t1.tokenize(contents):\n",
    "        parts = token.part_of_speech.split(',')\n",
    "        if('名詞' in parts\n",
    "            or '動詞' in parts\n",
    "            or '形容詞' in parts\n",
    "            or '形容動詞' in parts):\n",
    "            base = token.base_form\n",
    "            words.append(base)\n",
    "            rec = words_count.get(base)\n",
    "            if rec == None:\n",
    "                words_count[base] = 1\n",
    "            else:\n",
    "                words_count[base] = rec + 1\n",
    "    return words, words_count\n",
    "\n",
    "def univ_rev_lists(univs, uList):\n",
    "    univs_words = []\n",
    "    univs_words_count = []\n",
    "    for i, univ in enumerate(univs):\n",
    "        words = {}\n",
    "        words_count = {}\n",
    "        for group in univs[univ]:\n",
    "            words[group], words_count[group] = wakati_count_words(uList[i][group])\n",
    "        univs_words.append(words)\n",
    "        univs_words_count.append(words_count)\n",
    "    return univs_words, univs_words_count\n",
    "\n",
    "\n",
    "def sort_values(words):\n",
    "    return sorted(words.items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "def tf(words):\n",
    "    tf_words = {}\n",
    "    s = sum(words.values())\n",
    "    for key, value in words.items():\n",
    "        tf_words[key] = value / s\n",
    "    return tf_words\n",
    "\n",
    "def df(messages):\n",
    "    df_words = {}\n",
    "    for prof, mesg in messages.items():\n",
    "        for key in mesg.keys():\n",
    "            num = df_words.get(key)\n",
    "            if num == None:\n",
    "                df_words[key] = 1\n",
    "            else:\n",
    "                df_words[key] = num + 1\n",
    "    return df_words\n",
    "\n",
    "def tfidf(messages):\n",
    "    df_words = df(messages)\n",
    "    tf_tfidf = {}\n",
    "    size = len(messages)\n",
    "    for prof, mesg in messages.items():\n",
    "        tf_scores = tf(mesg)\n",
    "        tfidf_scores = {}\n",
    "        for key, value in tf_scores.items():\n",
    "            tfidf_scores[key] = value * math.log2(size / df_words[key])\n",
    "        tf_tfidf[prof] = (tf_scores, tfidf_scores)\n",
    "    return tf_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61e954",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 各大学、各学部のレビューにおける形態素解析済みの単語リストと単語出現回数リスト\n",
    "# revw[0]はmeijiのデータ　…\n",
    "revw, revwc = univ_rev_lists(univs_groups, univs_revLsit)\n",
    "\n",
    "with open(\"revw.pkl\", \"wb\") as f:\n",
    "    pickle.dump(revw, f)\n",
    "\n",
    "with open(\"revwc.pkl\", \"wb\") as f:\n",
    "    pickle.dump(revwc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3809189d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if-idfで特徴語抽出　学部名が含まれる語は除く\n",
    "\n",
    "all_revwc = [tfidf(revwc[i]) for i in range(len(univs_groups))]\n",
    "for i, univ in enumerate(univs_groups):\n",
    "    print(univs_name[univ][1])\n",
    "    for group in univs_groups[univ]:\n",
    "        words = list(all_revwc[i][group][1].keys())\n",
    "        for n in range(len(words)):\n",
    "            if words[n] in undergraduates_name[group]:\n",
    "                all_revwc[i][group][1][words[n]] = 0\n",
    "        print(undergraduates_name[group])\n",
    "        print(sort_values(all_revwc[i][group][1])[:5])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447c27b5",
   "metadata": {},
   "source": [
    "### 分散表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e20501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語にIDを付与\n",
    "def create_corpus(words, corpus = [], word_to_id = {}, id_to_word = {}):\n",
    "    corpus = []\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id: # 初めての単語なら\n",
    "            new_id = len(word_to_id)  # ID を決めて\n",
    "            word_to_id[word] = new_id # word_to_id に登録\n",
    "            id_to_word[new_id] = word # id_to_word に登録\n",
    "\n",
    "    corpus += [word_to_id[w] for w in words] # 文章をIDリストで表現\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "\n",
    "# 共起行列の計算\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix\n",
    "\n",
    "\n",
    "# コサイン類似度\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)\n",
    "\n",
    "\n",
    "# 似ている単語の検索\n",
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    if query not in word_to_id:\n",
    "        print('%s is not found' % query)\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53014f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "query = '楽しい'\n",
    "\n",
    "for i, univ in enumerate(univs_groups):\n",
    "    print(univs_name[univ][1])\n",
    "    for group in univs_groups[univ]:\n",
    "        corpus, word_to_id, id_to_word = create_corpus(revw[i][group])\n",
    "        word_matrix = create_co_matrix(corpus, len(word_to_id))\n",
    "        print(undergraduates_name[group])\n",
    "        most_similar(query, word_to_id, id_to_word, word_matrix, 3)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f87cd",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bfbd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 白ヤギさんが作った日本語word2vecモデル\n",
    "model_path = 'latest-ja-word2vec-gensim-model/word2vec.gensim.model'\n",
    "model = models.Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3170a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各大学学部のレビュー文を一つの文字列にまとめる\n",
    "reviews_txt = ''\n",
    "for i, univ in enumerate(univs_groups):\n",
    "    for group in univs_groups[univ]:\n",
    "        reviews_txt += univs_revLsit[i][group]\n",
    "\n",
    "# janomeで分かち書き\n",
    "tw = Tokenizer(wakati=True)\n",
    "token = tw.tokenize(reviews_txt)\n",
    "reviews_wakati = ' '.join(list(token))\n",
    "\n",
    "# 分かち書きされたテキストをファイルに書き込む\n",
    "with open('rev_wakati.txt', 'w', encoding='utf-8_sig') as f:\n",
    "    f.write(reviews_wakati)\n",
    "\n",
    "# レビュー文のword2vecのモデル作成\n",
    "docs = models.word2vec.LineSentence('rev_wakati.txt')\n",
    "rev_model = models.Word2Vec(docs, size=100, min_count=5, window=5, iter=3)\n",
    "rev_model.save('univ_review.model')\n",
    "\n",
    "\n",
    "# 白ヤギさんのモデルに各大学学部の分かち書きレビューモデルを追加学習\n",
    "docs = models.word2vec.LineSentence('rev_wakati.txt')\n",
    "model.train(docs, total_examples=model.corpus_count, epochs=model.iter)\n",
    "model.save('gensim_univs.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7d2ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大学学部のレビューの学習モデル\n",
    "rev_model = models.Word2Vec.load('univ_review.model')\n",
    "\n",
    "# 白ヤギ・学部レビューの融合学習モデル\n",
    "modelV2 = models.Word2Vec.load('gensim_univs.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819dde93",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '課題が少ない大学'\n",
    "w,wc = wakati_count_words(text)\n",
    "\n",
    "for x in w:\n",
    "    print(modelV2.wv.most_similar(x))\n",
    "    print()\n",
    "    print(rev_model.wv.most_similar(x))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa3c3a",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7cceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大学学部名のリスト化(最後尾はテスト用)、各レビュー文の分かち書きリスト(文書作成)\n",
    "\n",
    "univsName = []\n",
    "docs = []\n",
    "for i, u in enumerate(univs_groups):\n",
    "    univ = univs_name[u][1]\n",
    "    for g in univs_groups[u]:\n",
    "        undergraduate = undergraduates_name[g]\n",
    "        name = univ + ' ' + undergraduate\n",
    "        univsName.append(name)\n",
    "        docs.append(revw[i][g])\n",
    "univsName.append('testData')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考記事： http://qiita.com/okappy/items/32a7ba7eddf8203c9fa1\n",
    "\n",
    "# レビューに大学学部を付与\n",
    "class LabeledListSentence(object):\n",
    "    def __init__(self, words_list, labels):\n",
    "        self.words_list = words_list\n",
    "        self.labels = labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, words in enumerate(self.words_list):\n",
    "            yield models.doc2vec.LabeledSentence(words, ['%s' % self.labels[i]])\n",
    "\n",
    "\n",
    "sentences = LabeledListSentence(docs, univsName)\n",
    "\n",
    "revs_model = models.Doc2Vec(alpha=0.025, min_count=5, size=100, iter=20, workers=4)\n",
    "\n",
    "revs_model.build_vocab(sentences)\n",
    "\n",
    "revs_model.train(sentences, total_examples=revs_model.corpus_count, epochs=revs_model.iter)\n",
    "\n",
    "revs_model.save('univs_reviews.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b0aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "revs_model = models.Doc2Vec.load('univs_reviews.model')\n",
    "univsName = revs_model.docvecs.offset2doctag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# レビューに大学学部名を付与するためのクラス\n",
    "class LabeledListSentence(object):\n",
    "    def __init__(self, words_list, labels):\n",
    "        self.words_list = words_list\n",
    "        self.labels = labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, words in enumerate(self.words_list):\n",
    "            yield models.doc2vec.LabeledSentence(words, ['%s' % self.labels[i]])\n",
    "\n",
    "\n",
    "# 白ヤギ+レビューのword2vecモデルを用いて、単語の似ているワードをリスト化\n",
    "def similar_words(text, model, n):\n",
    "    words, words_count = wakati_count_words(text)\n",
    "    wordsList = []\n",
    "    for word in words:\n",
    "        sim_words = model.wv.most_similar(word, topn=n)\n",
    "        for w in range(len(sim_words)):\n",
    "            word = sim_words[w][0]\n",
    "            wordsList.append(word)\n",
    "    return wordsList\n",
    "\n",
    "\n",
    "\n",
    "# docsに入力となるテキストを新たに追加して、それを込みで学習させる\n",
    "def testData_train(data, docs, univs):\n",
    "    docs.append(data)\n",
    "    sentences = LabeledListSentence(docs, univs)\n",
    "    model = models.Doc2Vec(alpha=0.025, min_count=5, size=100, iter=20, workers=4)\n",
    "    model.build_vocab(sentences)\n",
    "    model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    univs = model.docvecs.offset2doctag\n",
    "    docs.pop()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大学学部とレビューのDoc2Vecのモデル構築と学習 \n",
    "# 繰り返し学習させる\n",
    "\n",
    "sentences = LabeledListSentence(docs, univsName)\n",
    "\n",
    "revs_model2 = models.Doc2Vec(sentences, dm=1, size=300, window=5, alpha=0.025, min_alpha=0.025,\n",
    "                            min_count=5, sample=1e-6)\n",
    "\n",
    "print('\\n訓練開始')\n",
    "for epoch in range(1000):\n",
    "    #print('Epoch: ' , epoch+1)\n",
    "    revs_model2.train(sentences, total_examples=revs_model2.corpus_count, epochs=revs_model2.iter)\n",
    "    revs_model2.alpha -= (0.025 - 0.0001) / 999\n",
    "    revs_model2.min_alpha = revs_model2.alpha\n",
    "print('\\n訓練終了')\n",
    "\n",
    "revs_model2.save('univs_reviews2.model')\n",
    "# 大学学部と各レビューのdoc2vecモデル\n",
    "revs_model2 = models.Doc2Vec.load('univs_reviews2.model')\n",
    "univsName = revs_model2.docvecs.offset2doctag\n",
    "input_text = 'プログラミング、画僧処理や映像処理、パソコンを勉強したい'\n",
    "text = similar_words(input_text, model, 20)\n",
    "\n",
    "# 新規文書のモデルにおけるベクトルを作成する\n",
    "input_vec = revs_model2.infer_vector(text)\n",
    "\n",
    "# 新規文書のベクトルと近いものを出力\n",
    "revs_model2.docvecs.most_similar([input_vec], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a987fdd",
   "metadata": {},
   "source": [
    "#### 実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49bac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '課題が少ない大学'\n",
    "test = similar_words(text)\n",
    "test_model = testData_train(test, docs, univsName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49661d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, wc = wakati_count_words(text)\n",
    "for t in w:\n",
    "    print(test_model.most_similar(t, topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8000375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 54の文書が入力したテキスト文\n",
    "# 理論上は、こちらが入力した条件(text)に最も近い(適切な)大学学部を出力する\n",
    "# 精度は不明\n",
    "\n",
    "test_model.docvecs.most_similar(54, topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2432a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73446834",
   "metadata": {},
   "source": [
    "### トピックモデル LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a0876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d9428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dictionary = Dictionary(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d393ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_corpus = [common_dictionary.doc2bow(text) for text in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca6f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(common_corpus, num_topics=5, id2word=common_dictionary.id2token, iterations=100000, gamma_threshold=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc9da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db76af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "str = '法政大学の市ヶ谷キャンパスには今年度から使用されている大内山校舎という新しい校舎があります。また、3年ほど前にできた富士見ゲートという校舎もあり、どちらも多くの授業が開講されているので、新しい校舎の綺麗な環境で勉強することができます。特にトイレが明るくとても綺麗で、混み合ってしまうという難点はありますが私はとても気に入っています。現在古い校舎は工事中なので移動が難しいですが、そちらが終わればとても使いやすい綺麗なキャンパスになると思います。'\n",
    "w,wc = wakati_count_words(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed6de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = common_dictionary.doc2bow(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f32a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda[vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a56dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.print_topic(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13cc15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
